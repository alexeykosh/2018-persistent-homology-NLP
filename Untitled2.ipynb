{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from textblob import TextBlob\n",
    "import operator\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import word2vec, KeyedVectors\n",
    "from persistent.persistent import PersistentPreTrained, PersistentNonTrained\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persistent homology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = lambda l: [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')\n",
    "texts = pd.read_csv('data/texts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts.TEXT = texts.TEXT.str.replace('[^\\w\\s]','')\n",
    "\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n",
    "\n",
    "texts['TEXT_L'] = texts.TEXT.apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_l = texts.TEXT_L\n",
    "\n",
    "texts_ph = []\n",
    "\n",
    "for element in texts_l:\n",
    "    try:\n",
    "        a = PersistentNonTrained(split_sent=element, min_count=1, workers=-1)\n",
    "        dgms = a.persistent()\n",
    "        texts_ph.append(flatten([[p.birth for p in dgms[1]], [p.death for p in dgms[1]]]))\n",
    "    except:\n",
    "        texts_ph.append([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1324</th>\n",
       "      <th>1325</th>\n",
       "      <th>1326</th>\n",
       "      <th>1327</th>\n",
       "      <th>1328</th>\n",
       "      <th>1329</th>\n",
       "      <th>1330</th>\n",
       "      <th>1331</th>\n",
       "      <th>1332</th>\n",
       "      <th>1333</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.003222</td>\n",
       "      <td>-0.003164</td>\n",
       "      <td>-0.002950</td>\n",
       "      <td>-0.002450</td>\n",
       "      <td>-0.002064</td>\n",
       "      <td>-0.001877</td>\n",
       "      <td>-0.001414</td>\n",
       "      <td>-0.001250</td>\n",
       "      <td>-0.001161</td>\n",
       "      <td>-0.001142</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.002950</td>\n",
       "      <td>-0.002280</td>\n",
       "      <td>-0.001803</td>\n",
       "      <td>-0.001697</td>\n",
       "      <td>-0.001533</td>\n",
       "      <td>-0.001326</td>\n",
       "      <td>-0.001286</td>\n",
       "      <td>-0.001275</td>\n",
       "      <td>-0.001250</td>\n",
       "      <td>-0.001082</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.002745</td>\n",
       "      <td>-0.001796</td>\n",
       "      <td>-0.001593</td>\n",
       "      <td>-0.001558</td>\n",
       "      <td>-0.001491</td>\n",
       "      <td>-0.001480</td>\n",
       "      <td>-0.001349</td>\n",
       "      <td>-0.001342</td>\n",
       "      <td>-0.001292</td>\n",
       "      <td>-0.000883</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.002950</td>\n",
       "      <td>-0.001385</td>\n",
       "      <td>-0.001349</td>\n",
       "      <td>-0.001141</td>\n",
       "      <td>-0.001125</td>\n",
       "      <td>-0.000859</td>\n",
       "      <td>-0.000820</td>\n",
       "      <td>-0.000806</td>\n",
       "      <td>-0.000684</td>\n",
       "      <td>-0.000616</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.001765</td>\n",
       "      <td>-0.001593</td>\n",
       "      <td>-0.001575</td>\n",
       "      <td>-0.001568</td>\n",
       "      <td>-0.001476</td>\n",
       "      <td>-0.001455</td>\n",
       "      <td>-0.001422</td>\n",
       "      <td>-0.001388</td>\n",
       "      <td>-0.001388</td>\n",
       "      <td>-0.001292</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.002293</td>\n",
       "      <td>-0.001691</td>\n",
       "      <td>-0.001258</td>\n",
       "      <td>-0.000558</td>\n",
       "      <td>-0.000552</td>\n",
       "      <td>-0.000448</td>\n",
       "      <td>-0.000198</td>\n",
       "      <td>-0.000159</td>\n",
       "      <td>-0.000058</td>\n",
       "      <td>-0.000041</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.002450</td>\n",
       "      <td>-0.001942</td>\n",
       "      <td>-0.001521</td>\n",
       "      <td>-0.001349</td>\n",
       "      <td>-0.001161</td>\n",
       "      <td>-0.001142</td>\n",
       "      <td>-0.000874</td>\n",
       "      <td>-0.000825</td>\n",
       "      <td>-0.000799</td>\n",
       "      <td>-0.000680</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.002785</td>\n",
       "      <td>-0.002052</td>\n",
       "      <td>-0.001153</td>\n",
       "      <td>-0.001147</td>\n",
       "      <td>-0.001142</td>\n",
       "      <td>-0.001085</td>\n",
       "      <td>-0.001072</td>\n",
       "      <td>-0.001032</td>\n",
       "      <td>-0.000891</td>\n",
       "      <td>-0.000882</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.002285</td>\n",
       "      <td>-0.001942</td>\n",
       "      <td>-0.001670</td>\n",
       "      <td>-0.001455</td>\n",
       "      <td>-0.001359</td>\n",
       "      <td>-0.001250</td>\n",
       "      <td>-0.000891</td>\n",
       "      <td>-0.000859</td>\n",
       "      <td>-0.000702</td>\n",
       "      <td>-0.000610</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.002544</td>\n",
       "      <td>-0.002089</td>\n",
       "      <td>-0.001869</td>\n",
       "      <td>-0.001842</td>\n",
       "      <td>-0.001730</td>\n",
       "      <td>-0.001697</td>\n",
       "      <td>-0.001431</td>\n",
       "      <td>-0.001396</td>\n",
       "      <td>-0.001388</td>\n",
       "      <td>-0.001376</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 1334 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0         1         2         3         4         5         6     \\\n",
       "0 -0.003222 -0.003164 -0.002950 -0.002450 -0.002064 -0.001877 -0.001414   \n",
       "1 -0.002950 -0.002280 -0.001803 -0.001697 -0.001533 -0.001326 -0.001286   \n",
       "2 -0.002745 -0.001796 -0.001593 -0.001558 -0.001491 -0.001480 -0.001349   \n",
       "3 -0.002950 -0.001385 -0.001349 -0.001141 -0.001125 -0.000859 -0.000820   \n",
       "4 -0.001765 -0.001593 -0.001575 -0.001568 -0.001476 -0.001455 -0.001422   \n",
       "5 -0.002293 -0.001691 -0.001258 -0.000558 -0.000552 -0.000448 -0.000198   \n",
       "6 -0.002450 -0.001942 -0.001521 -0.001349 -0.001161 -0.001142 -0.000874   \n",
       "7 -0.002785 -0.002052 -0.001153 -0.001147 -0.001142 -0.001085 -0.001072   \n",
       "8 -0.002285 -0.001942 -0.001670 -0.001455 -0.001359 -0.001250 -0.000891   \n",
       "9 -0.002544 -0.002089 -0.001869 -0.001842 -0.001730 -0.001697 -0.001431   \n",
       "\n",
       "       7         8         9     ...   1324  1325  1326  1327  1328  1329  \\\n",
       "0 -0.001250 -0.001161 -0.001142  ...    NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "1 -0.001275 -0.001250 -0.001082  ...    NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "2 -0.001342 -0.001292 -0.000883  ...    NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "3 -0.000806 -0.000684 -0.000616  ...    NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "4 -0.001388 -0.001388 -0.001292  ...    NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "5 -0.000159 -0.000058 -0.000041  ...    NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "6 -0.000825 -0.000799 -0.000680  ...    NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "7 -0.001032 -0.000891 -0.000882  ...    NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "8 -0.000859 -0.000702 -0.000610  ...    NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "9 -0.001396 -0.001388 -0.001376  ...    NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "\n",
       "   1330  1331  1332  1333  \n",
       "0   NaN   NaN   NaN   NaN  \n",
       "1   NaN   NaN   NaN   NaN  \n",
       "2   NaN   NaN   NaN   NaN  \n",
       "3   NaN   NaN   NaN   NaN  \n",
       "4   NaN   NaN   NaN   NaN  \n",
       "5   NaN   NaN   NaN   NaN  \n",
       "6   NaN   NaN   NaN   NaN  \n",
       "7   NaN   NaN   NaN   NaN  \n",
       "8   NaN   NaN   NaN   NaN  \n",
       "9   NaN   NaN   NaN   NaN  \n",
       "\n",
       "[10 rows x 1334 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_ph_df = pd.DataFrame(texts_ph)\n",
    "text_ph_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ph_df['ID'] = texts['ID']\n",
    "\n",
    "X = pd.merge(train, text_ph_df.fillna(0), on='ID', how='left').drop(['ID', 'CATEGORY'], 1).values\n",
    "y = pd.factorize(train['CATEGORY'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3825, 1334)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1148, 1334)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "logist = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_roc_auc(model, X_test, y_test):\n",
    "    fpr, tpr, _ = roc_curve(y_test, model.predict_proba(X_test)[:,1], pos_label=1)\n",
    "    auc_s = auc(fpr, tpr)\n",
    "    \n",
    "    return fpr, tpr, auc_s "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logist.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-Idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 2),\n",
    "    max_features=10000)\n",
    "\n",
    "text_tfidf = word_vectorizer.fit_transform(texts.TEXT).toarray()\n",
    "\n",
    "text_tfidf = pd.DataFrame(text_tfidf)\n",
    "\n",
    "text_tfidf['ID'] = texts['ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ti = pd.merge(train, text_tfidf, on='ID', how='left').drop(['ID', 'CATEGORY'], 1).values\n",
    "y_ti = pd.factorize(train['CATEGORY'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ti, X_test_ti, y_train_ti, y_test_ti = train_test_split(X_ti, y_ti, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logist_ti = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logist_ti.fit(X_train_ti, y_train_ti)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "bigram_vectorizer = CountVectorizer(ngram_range=(1, 2), token_pattern=r'\\b\\w+\\b', min_df=1)\n",
    "\n",
    "text_bow = bigram_vectorizer.fit_transform(texts.TEXT).toarray()\n",
    "\n",
    "text_bow = pd.DataFrame(text_bow)\n",
    "\n",
    "text_bow['ID'] = texts['ID']\n",
    "\n",
    "X_bow = pd.merge(train, text_bow, on='ID', how='left').drop(['ID', 'CATEGORY'], 1).values\n",
    "y_bow = pd.factorize(train['CATEGORY'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bow, X_test_bow, y_train_bow, y_test_bow = train_test_split(X_bow, y_bow, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc = SVC(probability=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc.fit(X_train_bow, y_train_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "lw = 2\n",
    "\n",
    "fpr, tpr, auc_s = get_roc_auc(logist, X_test, y_test)\n",
    "\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, \n",
    "         label=('Logitstic regression + w2v + PH (area under the curve:{0:0.2f})'.format(auc_s)))\n",
    "\n",
    "fpr, tpr, auc_s = get_roc_auc(logist_ti, X_test_ti, y_test_ti)\n",
    "\n",
    "plt.plot(fpr, tpr, color='red',\n",
    "         lw=lw, \n",
    "         label=('Logitstic regression + tf-idf (area under the curve:{0:0.2f})'.format(auc_s)))\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "\n",
    "fpr, tpr, auc_s = get_roc_auc(svc, X_test_bow, y_test_bow)\n",
    "\n",
    "plt.plot(fpr, tpr, color='blue',\n",
    "         lw=lw, \n",
    "         label=('SVC + BoW (area under the curve:{0:0.2f})'.format(auc_s)))\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_twit = pd.read_csv('data/Sentiment Analysis Dataset.csv', encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_twit.TEXT = data_twit.SentimentText.str.replace('[^\\w\\s]','')\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n",
    "\n",
    "data_twit['TEXT_L'] = data_twit.SentimentText.apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_twit = data_twit[0:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_twit.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persistent homology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_ph = []\n",
    "\n",
    "for element in data_twit['TEXT_L']:\n",
    "    try:\n",
    "        a = PersistentNonTrained(split_sent=element, min_count=1, workers=-1)\n",
    "        dgms = a.persistent()\n",
    "        texts_ph.append(flatten([[p.birth for p in dgms[1]], [p.death for p in dgms[1]]]))\n",
    "    except:\n",
    "        texts_ph.append([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ph = pd.DataFrame(texts_ph)\n",
    "\n",
    "X_twit = text_ph.fillna(0).values\n",
    "y_twit = data_twit.Sentiment[0:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_em = SVC(probability=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tw, X_test_tw, y_train_tw, y_test_tw = train_test_split(X_twit, y_twit, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_em.fit(X_train_tw, y_train_tw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_em.score(X_test_tw, y_test_tw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectorizer_2 = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 2),\n",
    "    max_features=10000)\n",
    "\n",
    "text_tfidf = word_vectorizer_2.fit_transform(data_twit.SentimentText).toarray()\n",
    "\n",
    "text_tfidf = pd.DataFrame(text_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tw_tfidf, X_test_tw_tfidf, y_train_tw_tfidf, y_test_tw_tfidf = train_test_split(text_tfidf, y_twit, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrc_em_tfidf = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrc_em_tfidf.fit(X_train_tw_tfidf, y_train_tw_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "lw = 2\n",
    "\n",
    "fpr, tpr, auc_s = get_roc_auc(svc_em, X_test_tw, y_test_tw)\n",
    "\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, \n",
    "         label=('Logitstic regression + w2v + PH (area under the curve:{0:0.2f})'.format(auc_s)))\n",
    "\n",
    "fpr, tpr, auc_s = get_roc_auc(lrc_em_tfidf, X_test_tw_tfidf, y_test_tw_tfidf)\n",
    "\n",
    "plt.plot(fpr, tpr, color='blue',\n",
    "         lw=lw, \n",
    "         label=('Logitstic regression + tf-idf (area under the curve:{0:0.2f})'.format(auc_s)))\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_twit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-44dd435e31c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbigram_vectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngram_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_pattern\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mr'\\b\\w+\\b'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtext_bow_tw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbigram_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_twit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentimentText\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtext_bow_tw_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_bow_tw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_twit' is not defined"
     ]
    }
   ],
   "source": [
    "bigram_vectorizer = CountVectorizer(ngram_range=(1, 2), token_pattern=r'\\b\\w+\\b', min_df=1)\n",
    "\n",
    "text_bow_tw = bigram_vectorizer.fit_transform(data_twit.SentimentText).toarray()\n",
    "\n",
    "text_bow_tw_df = pd.DataFrame(text_bow_tw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tw_bow, X_test_tw_bow, y_train_tw_bow, y_test_tw_bow = train_test_split(text_bow_tw_df , y_twit, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrc_em_bow = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrc_em_bow.fit(X_train_tw_bow, y_train_tw_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "lw = 2\n",
    "\n",
    "fpr, tpr, auc_s = get_roc_auc(svc_em, X_test_tw, y_test_tw)\n",
    "\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, \n",
    "         label=('SVC + w2v + PH (area under the curve:{0:0.2f})'.format(auc_s)))\n",
    "\n",
    "fpr, tpr, auc_s = get_roc_auc(lrc_em_tfidf, X_test_tw_tfidf, y_test_tw_tfidf)\n",
    "\n",
    "plt.plot(fpr, tpr, color='blue',\n",
    "         lw=lw, \n",
    "         label=('Logistic regression + tf-idf (area under the curve:{0:0.2f})'.format(auc_s)))\n",
    "\n",
    "fpr, tpr, auc_s = get_roc_auc(lrc_em_bow, X_test_tw_bow, y_test_tw_bow)\n",
    "\n",
    "plt.plot(fpr, tpr, color='green',\n",
    "         lw=lw, \n",
    "         label=('Logitstic regression + BoW (area under the curve:{0:0.2f})'.format(auc_s)))\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec vs. PH + word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_l = texts.TEXT_L\n",
    "\n",
    "texts_w2v = []\n",
    "\n",
    "for element in texts_l:\n",
    "    try:\n",
    "        a = PersistentNonTrained(split_sent=element, min_count=1, workers=-1)\n",
    "        texts_w2v.append(flatten(a._get_vectors()))\n",
    "    except TypeError:\n",
    "        texts_w2v.append([])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>4990</th>\n",
       "      <th>4991</th>\n",
       "      <th>4992</th>\n",
       "      <th>4993</th>\n",
       "      <th>4994</th>\n",
       "      <th>4995</th>\n",
       "      <th>4996</th>\n",
       "      <th>4997</th>\n",
       "      <th>4998</th>\n",
       "      <th>4999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000979</td>\n",
       "      <td>-0.000035</td>\n",
       "      <td>-0.002686</td>\n",
       "      <td>0.003769</td>\n",
       "      <td>0.000655</td>\n",
       "      <td>-0.001967</td>\n",
       "      <td>0.002269</td>\n",
       "      <td>0.004164</td>\n",
       "      <td>0.001035</td>\n",
       "      <td>0.001112</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000979</td>\n",
       "      <td>-0.000035</td>\n",
       "      <td>-0.002686</td>\n",
       "      <td>0.003769</td>\n",
       "      <td>0.000655</td>\n",
       "      <td>-0.001967</td>\n",
       "      <td>0.002269</td>\n",
       "      <td>0.004164</td>\n",
       "      <td>0.001035</td>\n",
       "      <td>0.001112</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000979</td>\n",
       "      <td>-0.000035</td>\n",
       "      <td>-0.002686</td>\n",
       "      <td>0.003769</td>\n",
       "      <td>0.000655</td>\n",
       "      <td>-0.001967</td>\n",
       "      <td>0.002269</td>\n",
       "      <td>0.004164</td>\n",
       "      <td>0.001035</td>\n",
       "      <td>0.001112</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.003214</td>\n",
       "      <td>0.000578</td>\n",
       "      <td>-0.001153</td>\n",
       "      <td>-0.003117</td>\n",
       "      <td>-0.002925</td>\n",
       "      <td>0.004268</td>\n",
       "      <td>0.004769</td>\n",
       "      <td>-0.003067</td>\n",
       "      <td>-0.000552</td>\n",
       "      <td>-0.002599</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000979</td>\n",
       "      <td>-0.000035</td>\n",
       "      <td>-0.002686</td>\n",
       "      <td>0.003769</td>\n",
       "      <td>0.000655</td>\n",
       "      <td>-0.001967</td>\n",
       "      <td>0.002269</td>\n",
       "      <td>0.004164</td>\n",
       "      <td>0.001035</td>\n",
       "      <td>0.001112</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000979</td>\n",
       "      <td>-0.000035</td>\n",
       "      <td>-0.002686</td>\n",
       "      <td>0.003769</td>\n",
       "      <td>0.000655</td>\n",
       "      <td>-0.001967</td>\n",
       "      <td>0.002269</td>\n",
       "      <td>0.004164</td>\n",
       "      <td>0.001035</td>\n",
       "      <td>0.001112</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000979</td>\n",
       "      <td>-0.000035</td>\n",
       "      <td>-0.002686</td>\n",
       "      <td>0.003769</td>\n",
       "      <td>0.000655</td>\n",
       "      <td>-0.001967</td>\n",
       "      <td>0.002269</td>\n",
       "      <td>0.004164</td>\n",
       "      <td>0.001035</td>\n",
       "      <td>0.001112</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.003214</td>\n",
       "      <td>0.000578</td>\n",
       "      <td>-0.001153</td>\n",
       "      <td>-0.003117</td>\n",
       "      <td>-0.002925</td>\n",
       "      <td>0.004268</td>\n",
       "      <td>0.004769</td>\n",
       "      <td>-0.003067</td>\n",
       "      <td>-0.000552</td>\n",
       "      <td>-0.002599</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000979</td>\n",
       "      <td>-0.000035</td>\n",
       "      <td>-0.002686</td>\n",
       "      <td>0.003769</td>\n",
       "      <td>0.000655</td>\n",
       "      <td>-0.001967</td>\n",
       "      <td>0.002269</td>\n",
       "      <td>0.004164</td>\n",
       "      <td>0.001035</td>\n",
       "      <td>0.001112</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.002956</td>\n",
       "      <td>0.001661</td>\n",
       "      <td>-0.001570</td>\n",
       "      <td>-0.001455</td>\n",
       "      <td>0.002018</td>\n",
       "      <td>0.002536</td>\n",
       "      <td>0.003637</td>\n",
       "      <td>-0.001786</td>\n",
       "      <td>0.003346</td>\n",
       "      <td>-0.004281</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 5000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0         1         2         3         4         5         6     \\\n",
       "0  0.000979 -0.000035 -0.002686  0.003769  0.000655 -0.001967  0.002269   \n",
       "1  0.000979 -0.000035 -0.002686  0.003769  0.000655 -0.001967  0.002269   \n",
       "2  0.000979 -0.000035 -0.002686  0.003769  0.000655 -0.001967  0.002269   \n",
       "3 -0.003214  0.000578 -0.001153 -0.003117 -0.002925  0.004268  0.004769   \n",
       "4  0.000979 -0.000035 -0.002686  0.003769  0.000655 -0.001967  0.002269   \n",
       "5  0.000979 -0.000035 -0.002686  0.003769  0.000655 -0.001967  0.002269   \n",
       "6  0.000979 -0.000035 -0.002686  0.003769  0.000655 -0.001967  0.002269   \n",
       "7 -0.003214  0.000578 -0.001153 -0.003117 -0.002925  0.004268  0.004769   \n",
       "8  0.000979 -0.000035 -0.002686  0.003769  0.000655 -0.001967  0.002269   \n",
       "9 -0.002956  0.001661 -0.001570 -0.001455  0.002018  0.002536  0.003637   \n",
       "\n",
       "       7         8         9     ...   4990  4991  4992  4993  4994  4995  \\\n",
       "0  0.004164  0.001035  0.001112  ...    NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "1  0.004164  0.001035  0.001112  ...    NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "2  0.004164  0.001035  0.001112  ...    NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "3 -0.003067 -0.000552 -0.002599  ...    NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "4  0.004164  0.001035  0.001112  ...    NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "5  0.004164  0.001035  0.001112  ...    NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "6  0.004164  0.001035  0.001112  ...    NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "7 -0.003067 -0.000552 -0.002599  ...    NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "8  0.004164  0.001035  0.001112  ...    NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "9 -0.001786  0.003346 -0.004281  ...    NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "\n",
       "   4996  4997  4998  4999  \n",
       "0   NaN   NaN   NaN   NaN  \n",
       "1   NaN   NaN   NaN   NaN  \n",
       "2   NaN   NaN   NaN   NaN  \n",
       "3   NaN   NaN   NaN   NaN  \n",
       "4   NaN   NaN   NaN   NaN  \n",
       "5   NaN   NaN   NaN   NaN  \n",
       "6   NaN   NaN   NaN   NaN  \n",
       "7   NaN   NaN   NaN   NaN  \n",
       "8   NaN   NaN   NaN   NaN  \n",
       "9   NaN   NaN   NaN   NaN  \n",
       "\n",
       "[10 rows x 5000 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_w2v = pd.DataFrame(texts_w2v)\n",
    "text_w2v.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_w2v['ID'] = texts['ID']\n",
    "\n",
    "X = pd.merge(train, text_w2v.fillna(0), on='ID', how='left').drop(['ID', 'CATEGORY'], 1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3825, 5000)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_w, X_test_w, y_train_w, y_test_w = train_test_split(X, pd.factorize(train['CATEGORY'])[0], test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "logist_w = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logist_w.fit(X_train_w, y_train_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76655052264808365"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logist_w.score(X_test_w, y_test_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmkAAAGtCAYAAABN4OPtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAEMBF9Tgzv+fR5cAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x122faef60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "lw = 2\n",
    "\n",
    "fpr, tpr, auc_s = get_roc_auc(logist, X_test, y_test)\n",
    "\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, \n",
    "         label=('Logitstic regression + w2v + PH (area under the curve:{0:0.2f})'.format(auc_s)))\n",
    "\n",
    "\n",
    "fpr, tpr, auc_s = get_roc_auc(logist_w, X_test_w, y_test_w)\n",
    "\n",
    "plt.plot(fpr, tpr, color='blue',\n",
    "         lw=lw, \n",
    "         label=('Logitstic regression + w2v (area under the curve:{0:0.2f})'.format(auc_s)))\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80201718341426975"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logist.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format('persistent/GoogleNews-vectors'\n",
    "                                              '-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
